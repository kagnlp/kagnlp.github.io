---
title: What It Takes to Control Societal Bias in NLP
share: "true"
layout: page
css: "/css/academicons.css"
---

### Abstract
Natural language processing techniques play important roles in our daily life. Despite these methods being successful in various applications, they run the risk of exploiting and reinforcing the societal biases (e.g. gender bias) that are present in the underlying data. For instance, an automatic resume filtering system may inadvertently select candidates based on their gender and race due to implicit associations between applicant names and job titles, causing the system to perpetuate unfairness potentially. In this talk, I will describe a collection of results that quantify and control implicit societal biases in a wide spectrum of vision and language tasks, including word embeddings, coreference resolution, and visual semantic role labeling. These results lead to greater control of NLP systems to be socially responsible and accountable.

### Slides
[.pdf]({{site.baseurl}}/documents/slides/genderbias.pdf)

### Related Papers
<div style="display:none">
{% cite zhou2019examining shi2019retrofitting sheng2019woman ahmad2019crosslingual meng2019target wang2019balanced sun2019debiasing ahmad2019crosslingual zhao2019gender alzanto2018generating zhao2018learning zhao2018gender zhao2017men bolukbasi2016man %}
</div>
{% bibliography --cited %}
